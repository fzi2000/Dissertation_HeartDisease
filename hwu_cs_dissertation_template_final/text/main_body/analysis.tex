\section{Evaluation Strategy }
\label{sec:eval}

This chapter details the evaluation metrics used to test the performance of the developed \gls{cvd} prediction model. The metrics used are mentioned below along with their purpose and formula.

\subsection{Performance Metrics}
\label{sec:eval_1}
\textbf{\gls{tp}}- The cases that are correctly classified as positive results. \\
\textbf{\gls{fp}}- The cases that are predicted as positive but were negative. \\
\textbf{\gls{tn}}- The cases that are correctly predicted as negative. \\
\textbf{\gls{fn}}- The cases that were actually positive but predicted as negative.\\

\subsubsection{Accuracy} \leavevmode
\label{sec:eval_2}
\\The ratio of correctly classified predictions by the total number of cases examined. \\
Purpose: This metric highlights the percentage of outcomes that are correctly classified. \citep{devi2021deep}

\noindent Formula: \vspace{-1.9 em}
\begin{center}
 \[
\text{Accuracy} = \frac{(TP + TN)}{(TP + TN + FP + FN)}  \vspace{0.3 cm}
\]
\end{center}


\subsubsection{Precision} \leavevmode
\label{sec:eval_2}
\\The ratio of true positive predictions divided by the total predicted positives. \\
Purpose: It demonstrates the relevance of the results. \\
\noindent Formula: \vspace{-1.9 em}
\begin{center}
 \[
\text{Precision} = \frac{(TP)}{(TP + FP)} \vspace{0.3cm}
\]
\end{center}  

\subsubsection{Recall (Sensitivity or True Positive Rate)} \leavevmode
\\The ratio of true positive predictions predicted by the model to the total actual positives.\\
Purpose: It presents the model’s ability to find relevant results. A high recall indicates that at-risk patients are not disregarded.\\
\noindent Formula: \vspace{-1.9 em}
\begin{center}
\[
\text{Recall} = \frac{(TP)}{(TP + FN)} \vspace{0.3cm}
\]
\end{center} 

\subsubsection{F1 Score} \leavevmode
\\The harmonic mean of precision and recall, offers a balance between the two metrics. \\
Purpose: It is useful when the class distribution is imbalanced.\\
\noindent Formula: \vspace{-1.9 em}
\begin{center}
\[
F1\ \text{Score} = \frac{2 \times (\text{Precision} \times \text{Recall})}{\text{Precision} + \text{Recall}} \vspace{0.3cm}
\] 
\end{center} 

\subsubsection{\gls{aucroc}} \leavevmode
\\AUC measures the model’s capacity to differentiate between positive and negative classes. \\
Purpose: It visualizes how well the model performs, especially in imbalanced datasets. The equation for AUC is given below where TPR denotes the True Positive Rate and FPR denotes the False Positive Rate. 
\[
\text{AUC} = \int_{0}^{1} \text{TPR}(\text{FPR}^{-1}(x)) \, dx
\]


\subsubsection{Confusion Matrix} \leavevmode
\\The confusion matrix is a table that plots the performance of a model by showing the true positives, true negatives, false positives, and false negatives. \\
Purpose: It helps to focus on the types of errors made by the model and refine performance.
% \vspace{0.3 cm}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicted True} & \textbf{Predicted False} \\
\hline
\textbf{Actual True} & True Positive (TP) & False Negative (FN) \\
\hline
\textbf{Actual False} & False Positive (FP) & True Negative (TN) \\
\hline
\end{tabular}
\caption{Confusion Matrix}
\label{tab:confusion_matrix}
\end{table}
\vspace{-0.9cm}
\subsection{Additional Evaluation Strategies} \leavevmode \vspace{-0.8cm}
\subsubsection{Cross-Validation} \leavevmode
\\Purpose: K-fold Cross-validation is used to assess a model’s performance more reliably by testing it across multiple data subsets. This ensures that the model is trained well and prevents overfitting. \\In k-fold cross-validation, the dataset is partitioned into k equal-sized segments. \citep{katabathina2024crossvalidation}. The model is then
trained on k-1 folds and tested on the remaining reserved fold. During the k repetitions of the process, each fold serves as the test set exactly once. The final metric is the average of the metrics from each fold, giving an overall average of the model’s performance.

\subsubsection{\gls{mae}} \leavevmode
\gls{mae} is known as the average (mean) of the absolute differences between the actual values and predicted values. Purpose: \gls{mae} provides a simple interpretation of prediction errors. 
\[
\text{MAE} = \frac{1}{m} \sum_{i=1}^{m} \left| y_i - y^*_i \right|
\]
where:
$y_i$ is the actual value, $y^*_i$ is the predicted value and $m$ is the total number of predictions.

\subsection{Interpretability and Explainability Assessment}\leavevmode
Tools like SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations) provide insights into how the model makes its predictions, which can improve trust and understanding in clinical settings. By visualising contributions to individual predictions, it helps to identify the most influential factors.